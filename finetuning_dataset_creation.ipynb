{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6085d0c-d5e6-4227-8395-450f4ecb33c4",
   "metadata": {},
   "source": [
    "# Creating training data from text for finetuning\n",
    "\n",
    "Here we are using Deepseek LLM in order to turn text into structured json which could later be used as training data for model finetuning.\n",
    "We have picked the most reputable sources for our model and now we need to turn it into a Q/A structured jsons in order to finetune it \n",
    "\n",
    "## What are we doing here ?\n",
    "\n",
    "We have 2 prompt structures:\n",
    "\n",
    "- for data from articles,videos and just good internet resources\n",
    "\n",
    "- for data from books\n",
    "\n",
    "\n",
    "## How did you get the data ?\n",
    "\n",
    "Articles: scraped data directly from the web from publically available resources\n",
    "\n",
    "Videos: took the transcripts from publically available data\n",
    "\n",
    "Books: Used text data from books that i have purchased prior. Used pdfplumber to extract text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6155d2a8-4f9c-46d1-b5e1-97a3a8f43608",
   "metadata": {},
   "source": [
    "## 1. Here we are writing prompts and the program for extracting Text data from articles and videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32de33c6-a7ef-4e09-8247-c7a0fe90fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_format = {\n",
    "    \"questions\": [\n",
    "        {\n",
    "          \"question\": \"clear, context-independent question\", \n",
    "          \"answer\": \"concise 1-3 sentence response\",\n",
    "          \"category\": \"topic category\",\n",
    "          \"source\": \"source document\"\n",
    "        },\n",
    "        {\n",
    "          \"question\": \"clear, context-independent question\", \n",
    "          \"answer\": \"concise 1-3 sentence response\",\n",
    "          \"category\": \"topic category\",\n",
    "          \"source\": \"source document\"\n",
    "        }\n",
    "      ] \n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "examples = \"\"\"\n",
    "Input Text:\n",
    "\"Bitcoin transactions are verified by miners through proof-of-work consensus.\"\n",
    "\"Ethereum's smart contracts are self-executing agreements written in Solidity. They run on the EVM (Ethereum Virtual Machine) and enforce terms without intermediaries.\"\n",
    "\"Proof-of-Stake (PoS) validators are chosen to create new blocks based on the amount of cryptocurrency they 'stake' as collateral. This reduces energy consumption compared to Proof-of-Work.\"\n",
    "\n",
    "JSON Output: {\n",
    "    \"questions\": [\n",
    "        {\n",
    "          \"question\": \"How do Bitcoin miners verify transactions?\",\n",
    "          \"answer\": \"Bitcoin miners verify transactions by solving complex cryptographic puzzles through proof-of-work consensus. Successful verification adds transactions to the blockchain.\",\n",
    "          \"category\": \"Blockchain Tech (Consensus)\",\n",
    "          \"source\": \"Bitcoin Whitepaper\"\n",
    "        },\n",
    "        {\n",
    "          \"question\": \"What are Ethereum smart contracts and how do they execute?\",\n",
    "          \"answer\": \"Ethereum smart contracts are self-executing agreements written in Solidity. They automatically enforce terms without intermediaries by running on the Ethereum Virtual Machine (EVM).\",\n",
    "          \"category\": \"Blockchain Tech (Smart Contracts)\",\n",
    "          \"source\": \"Ethereum Documentation\"\n",
    "        },\n",
    "        {\n",
    "          \"question\": \"How does Proof-of-Stake select validators and why is it energy-efficient?\",\n",
    "          \"answer\": \"Proof-of-Stake selects validators based on the amount of cryptocurrency they stake as collateral. It avoids energy-intensive computations, making it more efficient than Proof-of-Work.\",\n",
    "          \"category\": \"Blockchain Tech (Consensus)\",\n",
    "          \"source\": \"Consensus Research Papers\"\n",
    "        }\n",
    "\n",
    "      ] }\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628c5e46-e9f4-466c-80f9-0304abf83a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "  \n",
    "    Role: You are a Cryptocurrency Data Engineer specializing in creating flawless question-answer pairs for AI training. Your outputs must be technically precise, pedagogically structured, and free of hallucinations.are a Cryptocurrency Data Engineer specializing in creating flawless question-answer pairs for AI training. Your outputs must be tRole: You are a Cryptocurrency Data Engineer specializing in creating flawless question-answer pairs for AI training. Your outputs must be technically precise, pedagogically structured, and free of hallucinations.echnically precise, pedagogically structured, and free of hallucinations. \n",
    "    Output: Convert the input text into one or more Always return JSON array in this exact format:{answer_format}\n",
    "    Return as a JSON array if multiple facts/questions can be extracted:\n",
    "    Examples: {examples}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448bdd16-aa93-4ea1-b8c1-fd22aac25dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"APIKEY:)\", base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420b8e42-b45f-48c8-b0c1-b805045302e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "class QAItem(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "    category: str\n",
    "    source: str\n",
    "\n",
    "class QAOutput(BaseModel):\n",
    "    questions: List[QAItem]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d73db4-000b-489f-bcde-8056b9fa6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_json(system_prompt: str, file_path: str, output_path: str) -> QAOutput:\n",
    "    \"\"\"Process text file and generate validated JSON output using Pydantic\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        file_text = file.read()\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": file_text},\n",
    "            ],\n",
    "            response_format={'type': 'json_object'},\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        response_content = response.choices[0].message.content\n",
    "        \n",
    "        try:\n",
    "            json_data = json.loads(response_content)\n",
    "            validated_output = QAOutput.model_validate(json_data)\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "                out_file.write(validated_output.model_dump_json(indent=2))\n",
    "    \n",
    "            return validated_output\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Invalid JSON received for {file_path}: {str(e)}\")\n",
    "            raise ValueError(f\"Failed to parse LLM response as JSON: {response_content}\") from e\n",
    "        except ValidationError as e:\n",
    "            print(f\"Validation failed for {file_path}: {str(e)}\")\n",
    "            print(f\"Raw response content: {response_content}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5328e16-4f22-42c3-bfc8-fda45859d48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: video_text_2/two.txt -> Saved to answer_videos_2/two.json\n",
      "Completed: video_text_2/four.txt -> Saved to answer_videos_2/four.json\n",
      "Completed: video_text_2/five.txt -> Saved to answer_videos_2/five.json\n",
      "Completed: video_text_2/three.txt -> Saved to answer_videos_2/three.json\n",
      "Completed: video_text_2/six.txt -> Saved to answer_videos_2/six.json\n",
      "Completed: video_text_2/one.txt -> Saved to answer_videos_2/one.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "answer_folder = 'answer_videos_2'\n",
    "os.makedirs(answer_folder, exist_ok=True)\n",
    "\n",
    "file_path = 'video_text_2'\n",
    "\n",
    "for filename in os.listdir(file_path):\n",
    "    input_path = os.path.join(file_path, filename)\n",
    "    \n",
    "    output_filename = os.path.splitext(filename)[0] + '.json'\n",
    "    output_path = os.path.join(answer_folder, output_filename)\n",
    "    \n",
    "    text_json(system_prompt, input_path, output_path)\n",
    "    print(f\"Completed: {input_path} -> Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820cd63-7e00-4193-83c1-c3a4be6a2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# file_path = 'Coinbase_tutorials'\n",
    "# output_path = 'answer.json'\n",
    "# for filename in os.listdir(file_path):\n",
    "#     final_path = file_path+\"/\"+filename\n",
    "#     text_json(system_prompt,final_path,output_path)\n",
    "#     print(f\"completed: {final_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74263d22-12b7-4bd8-96f0-8ea25e4e5060",
   "metadata": {},
   "source": [
    "## 2. Here we extract from books\n",
    "\n",
    "We are using everything similar except the pydantic model , examples and prompts are different. Basically we just adjusted the returned data and the LLM instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3074f217-f63d-43fc-991d-4539a897afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_format = {\n",
    "    \"questions\": [\n",
    "        {\n",
    "          \"question\": \"clear, context-independent question\", \n",
    "          \"answer\": \"concise 1-3 sentence response\",\n",
    "          \"page\": \"which pages in the book did you get these\",\n",
    "          \"book\": \"source document or which book is it \"\n",
    "        },\n",
    "        {\n",
    "          \"question\": \"clear, context-independent question\", \n",
    "          \"answer\": \"concise 1-3 sentence response\",\n",
    "          \"page\": \"which pages in the book did you get these\",\n",
    "          \"book\": \"source document or which book is it \"\n",
    "        }\n",
    "      ] \n",
    "            }\n",
    "\n",
    "\n",
    "examples = \"\"\"\n",
    "Input Text:\n",
    "\"Bitcoin transactions are verified by miners through proof-of-work consensus.\"\n",
    "\"Ethereum's smart contracts are self-executing agreements written in Solidity. They run on the EVM (Ethereum Virtual Machine) and enforce terms without intermediaries.\"\n",
    "\"Proof-of-Stake (PoS) validators are chosen to create new blocks based on the amount of cryptocurrency they 'stake' as collateral. This reduces energy consumption compared to Proof-of-Work.\"\n",
    "\n",
    "JSON Output: {\n",
    "    \"questions\": [\n",
    "        {\n",
    "          \"question\": \"How do Bitcoin miners verify transactions?\",\n",
    "          \"answer\": \"Bitcoin miners verify transactions by solving complex cryptographic puzzles through proof-of-work consensus. Successful verification adds transactions to the blockchain.\",\n",
    "          \"page\": \"5\",\n",
    "          \"book\": \"Bitcoin Whitepaper\"\n",
    "        },\n",
    "        {\n",
    "          \"question\": \"What are Ethereum smart contracts and how do they execute?\",\n",
    "          \"answer\": \"Ethereum smart contracts are self-executing agreements written in Solidity. They automatically enforce terms without intermediaries by running on the Ethereum Virtual Machine (EVM).\",\n",
    "          \"page\": \"2-3\",\n",
    "          \"book\": \"Ethereum Documentation\"\n",
    "        },\n",
    "        {\n",
    "          \"question\": \"How does Proof-of-Stake select validators and why is it energy-efficient?\",\n",
    "          \"answer\": \"Proof-of-Stake selects validators based on the amount of cryptocurrency they stake as collateral. It avoids energy-intensive computations, making it more efficient than Proof-of-Work.\",\n",
    "          \"page\": \"4-6\",\n",
    "          \"book\": \"Consensus Research Papers\"\n",
    "        }\n",
    "\n",
    "      ] }\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9094dfea-50dd-487b-b798-7d89236fc0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "  \n",
    "    Role: You are a Cryptocurrency Data Engineer specializing in creating flawless question-answer pairs for AI training. Your outputs must be technically precise, pedagogically structured, and free of hallucinations.are a Cryptocurrency Data Engineer specializing in creating flawless question-answer pairs for AI training. Your outputs must be tRole: You are a Cryptocurrency Data Engineer specializing in creating flawless question-answer pairs for AI training. Your outputs must be technically precise, pedagogically structured, and free of hallucinations.echnically precise, pedagogically structured, and free of hallucinations. \n",
    "    Output: Convert the input text into one or more Always return JSON array in this exact format:{answer_format}\n",
    "    Return as a JSON array if multiple facts/questions can be extracted:\n",
    "    Examples: {examples}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45eda02e-0dc6-4e75-85fb-221b74e88f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from pydantic import BaseModel, ValidationError, Field\n",
    "\n",
    "class QAItem(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "    page : str\n",
    "    book: str = Field(description=\"which book was this information from\")\n",
    "\n",
    "    \n",
    "class QAOutput(BaseModel):\n",
    "    questions: List[QAItem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9394b570-45bb-403a-baac-a8052ed39b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: crypto_books_text/the-infinite-machine_part2.txt -> Saved to crypto_books_dataset/the-infinite-machine_part2.json\n",
      "Completed: crypto_books_text/the-infinite-machine_part5.txt -> Saved to crypto_books_dataset/the-infinite-machine_part5.json\n",
      "Completed: crypto_books_text/the-infinite-machine_part4.txt -> Saved to crypto_books_dataset/the-infinite-machine_part4.json\n"
     ]
    }
   ],
   "source": [
    "answer_folder = 'crypto_books_dataset'\n",
    "os.makedirs(answer_folder, exist_ok=True)\n",
    "\n",
    "file_path = 'crypto_books_text'\n",
    "\n",
    "for filename in os.listdir(file_path):\n",
    "    input_path = os.path.join(file_path, filename)\n",
    "    \n",
    "    output_filename = os.path.splitext(filename)[0] + '.json'\n",
    "    output_path = os.path.join(answer_folder, output_filename)\n",
    "    \n",
    "    text_json(system_prompt, input_path, output_path)\n",
    "    print(f\"Completed: {input_path} -> Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f130d96f-2b57-4216-a24f-78cccc3c6ddc",
   "metadata": {},
   "source": [
    "# Here is the script with how i extracted the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74479071-e124-478a-9fcc-23ac8805b319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def split_pdf_to_txt(input_folder: str, output_folder: str, pages_per_file: int = 50):\n",
    "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for filename in os.listdir(input_folder):\n",
    "        if not filename.endswith('.pdf'):\n",
    "            continue\n",
    "            \n",
    "        pdf_path = os.path.join(input_folder, filename)\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            all_texts = [page.extract_text() for page in pdf.pages]\n",
    "            \n",
    "            part_number = 1\n",
    "            for i in range(0, len(all_texts), pages_per_file):\n",
    "                chunk = all_texts[i:i+pages_per_file]\n",
    "                chunk_text = \"\\n\\n\".join(chunk)  \n",
    "                \n",
    "                output_filename = f\"{base_name}_part{part_number}.txt\"\n",
    "                output_path = os.path.join(output_folder, output_filename)\n",
    "                \n",
    "                with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(chunk_text)\n",
    "                \n",
    "                part_number += 1 \n",
    "\n",
    "\n",
    "split_pdf_to_txt(\n",
    "    input_folder='books_pdf',\n",
    "    output_folder='crypto_books_dataset'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
